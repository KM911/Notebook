熵 (entropy) : 描述系统混乱程度的量. 
信息 : 用于消除不确定的量. 

数量相等 , 意义相反 . 获取信息就是消除熵. 


单项选择题 

一个单项选择题,如果
就是你的还是的可能性有4种, 为了消除全部的不确定性,需要的信息量为

## 如何定量计算熵/信息

现在有一个硬币, 我想要知道其是正面还是方面,如何表达? 
0 表示反面, 1 表示正面. 

现在你要给同学传递单选题的答案,你该如何表达? 
00 01 10 11 分别表示A,B,C,D 

你很快就可以得到一个公式对于存在N种可能的结果,需要 $log_2{N}$ bit来表示. 

## 如何是可能性不一样的呢? 

我们可以将 p = 0.01 看作是 从100个球中抽一个球,这样就变成了上面的表达式了,我可太能够理解了,不是吗? 当然了这里只是理论的极限. 


$$H = \sum{pi\log{2}{\frac{1}{p_i}}}$$

对于上面的问题,因为每个选择只有两种状态, (是正确答案和不是正确答案) , 并且当四个选项的可能性相等,此时的不缺确定性最大.

底数作为状态,上面的数字叫什么? 
$E= log_{2}{4} = 2$

$H = \sum{p}{log{p}}$
概率和熵


## 误区 

在思考问题的时候,会不经意间切换


### 利用信息论来解决问题 

1000瓶药水，1瓶有毒药，几只小白鼠能够找出？

这个问题其实和上面的问题是一样的, 有一个选择题有1000个选项,需要几个bit位? 
$$H = log_{2}{1000} = 9.96 $$ 

所以我们需要10只老鼠才可以. 但是问题在于这里我没有表示出具体的喂药步骤不是吗? 
如何表示. 其实就变成了如何进行拼凑的? 

建设我要做到 

TODO 计算出喂药问题 