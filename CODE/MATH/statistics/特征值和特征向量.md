
对协方差矩阵的特征向量最直观的解释之一是：它总是指向数据方差最大的方向。

看不明白对吧,我也看不明白,我们现在找一组数据,让你体会一下. 

>[!exp] y=2x 直线 
> ```r
> x<- c(1,2,3,4,5)
> y<- c(2,4,6,8,10)
> Z<- cbind(x,y)
> 
> # 输出协方差矩阵
> cov(Z)
> # 计算特征值
> eigen(cov(Z))$values
> # 输出特征向量 
> eigen(cov(Z))$vectors
> ```

你可以得到两个特征向量,绘图
你可能觉得这个好像没有什么特别的,因为这个数据不太"数学"

>[!example] 我们计算一下 $\frac{1}{\sqrt{5}}$ 
>```r
>1/sqrt(5) 
>```

>[!note] 特征向量的含义 
>所以我们上面得到的特征向量其实是(1,2)和(-2,1),你会发现第一个特征向量的方向就是我们数据的方向. 
>此外两者相互垂直. 

>[!note] 特征值
>特征值数据大小无意义,占比有意义.上面的特征值是 12.5和0,说明第一特征值对应的特征向量可以完全表示这些数据. 

>[!example]- y=3x 更多的案例进行验证
>这次的特征向量应该为(1,3) 即 $(\frac{1}{\sqrt{10}},\frac{3}{\sqrt{10}})$
>```r
>x<- c(1,2,3,4,5)
y<- c(3,6,9,12,15)
Z<- cbind(x,y)
> 
> # 输出协方差矩阵
> cov(Z)
> # 计算特征值
> eigen(cov(Z))$values
> # 输出特征向量 
> eigen(cov(Z))$vectors
> 1/sqrt(10)
>```



>[!example] 添加误差,刚刚的数据太完美了,添加一点误差,看看特征值的变化. 
>```r
>x<- c(1,2,3,4,5,6)
y<- c(2,4,6,8,10,11)
Z<- cbind(x,y)
> 
> # 输出协方差矩阵
> cov(Z)
> # 计算特征值
> eigen(cov(Z))$values
> # 输出特征向量 
> eigen(cov(Z))$vectors
>```

>[!note] 变化解释
> 结果和上面类似,因为我们的数据总体趋势还是 y=2x,所以特征值和特征向量变化不大. 同时第二大特征值也不再为零,说明如果你想要保留数据全部原始信息,你需要第二特征向量. 


>[!note] 特征向量和线性变化 
>特征向量出来了,可以将数据进行线性变化,旋转, 使得其大部分都落到新的x轴上,这样就可以实现降维的目的. 


## 应用 

>[!note] 主成分分析
>可以说主成分分析就是在利用特征值和特征向量的意义在做说明. 

优化线性回归,利用主成分来进行线性回归而不是
?仅仅利用旋转,真的可以获得更加合理的线性回归分析结果吗? 我应该拿一些数据来说明试试看.




直到现在我才开始瞥见一点眉目,太不容易了. 

## 特征向量的计算公式为什么是这个

这个难度对于我们来说太高了. 
