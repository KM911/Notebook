hierarchical clustering
## 聚类分析

>[!faq] 聚合 和 分类 是一样的吗? 
>如果从机器学习的角度来区分,聚类是无监督学习,后者是有监督学习. 即分类的标准和依据如果给出了,就是分类,从无标准的数据中,找出其相似性,就是聚类. 

>[!exp] 学生考试数据
>我们将考试分数低于60%的学生定义为考试不及格,可以将学生分为两类. 这是分类问题. 
>我们要求从学生成绩中,分出良好,优秀,中等 三个等级. 这是聚类问题. 

>[!tip] 是否存在硬性指标 可以非常快速判断其属于哪一类问题. 




>[!faq] 如何进行聚类
>虽然聚类问题,没有给出分类标准,但是我们肯定还是要将其转化为一个可以定量计算的问题. 问题的可求解性


>[!note] 两点之间的距离 作为相似度的度量 定量分析
>两个数据点$R_1{(a_1 , a_2, a_3,...a_n)}$,$R_2{(b_1 , b_2, b_3,...b_n)}$, 我们就可以计算他们的距离
>

>[!tip] 计算距离的公式. 
>1.欧式距离 (欧几里和距离) 坐标轴的距离计算方法. 
>$D = \sqrt{(a_1-b_1)^2 + (a_2-b_2)^2+...(a_n-b_n)^2 }$
>欧式距离的大小和各个数据的单位有关,因为其是不同的量. 

![[多元统计分析-20231211151241168.webp]]
曼哈顿距离

切比雪夫距离
![[多元统计分析-20231211144535585.webp]]


### 递推公式计算分层聚类 

>[!note] 对数据进行分类,如果不是年龄大于40这种隐形要求指标,我们是无法"分类"的其实. 
>所以分类分析中非常重要的内容是,确定将结果分成几类. 

#### 类与类之间的距离计算

> [!note] 最小距离 : 将两个类中最小的距离作为两个类的距离
> 最大距离 : 类比 
> 类平均距离 : 计算所有的可能
> 重心距离 : 重心其实就是类的平局值之间的距离
> 离差平方法 :  


#### 例题 按照最小距离法将 1 2 6.5 7 11 分类

1. 单独视为一类, 计算类间距离. 可以得到矩阵 (自己和自己的距离为0) 这是一个对称的矩阵. 
![[多元统计分析-20231211145605687.webp]]

2.开始进行合并, 将类间距离最小的进行合并. 

可以看出, G3和G4两点距离最近,我们将其合并,可以得到 G1 = {1} G2={2} G5={11} G6={6.5,7} 

再次计算他们的距离矩阵,可以得到. 

![[多元统计分析-20231211145902344.webp]]

3.重复第二步,直到合并为只有两类. 

这里是G1和G2的距离最近,将其合并,得到 G5={11} G6= {6.5,7} G7={1,2} 

可以计算出距离矩阵. 

![[多元统计分析-20231211150314790.webp]]

#### 代码 complete linkage

```r
x <- c(1,2,6.5,7,11); 
dim(x) = c(5,1)
d = dist(x)
hc2 = hclust(d, method="complete") 
# 和绘图无关 这里是解释如何合并的过程 
# cbind(hc2$merge, hc2$height)
dend2 = as.dendrogram(hc2)
plot(dend2,horiz = TRUE)
```
### K-means 聚类方法 

![[多元统计分析-20231211150808390.webp]]


#### 评价指标 

![[多元统计分析-20231211150644850.webp]]![[多元统计分析-20231211150657291.webp]]



